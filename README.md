# AI & ML Internship â€“ Task 5  
## Train-Test Split & Evaluation Metrics

## ğŸ“Œ Objective
The objective of this task is to understand the importance of train-test splitting and to evaluate a machine learning classification model using standard evaluation metrics. The Heart Disease Dataset is used to train and test a Logistic Regression model.

---

## ğŸ“ Dataset
- **Name:** Heart Disease Dataset  
- **Type:** Binary Classification  
- **Target Variable:** `target`  
  - `0` â†’ No Heart Disease  
  - `1` â†’ Presence of Heart Disease  

---

## ğŸ›  Tools & Libraries Used
- Google Collab
---

## ğŸ” Steps Performed
1. Imported required Python libraries for data processing and model evaluation.  
2. Loaded and explored the Heart Disease dataset.  
3. Separated independent features and the target variable.  
4. Split the dataset into training and testing sets using an 80:20 ratio.  
5. Trained a Logistic Regression model on the training data.  
6. Made predictions on the test data.  
7. Evaluated the model using accuracy, precision, recall, and confusion matrix.  
8. Interpreted the evaluation results to understand model performance.

---

## ğŸ“Š Evaluation Metrics
- **Accuracy:** Measures the overall correctness of the model.  
- **Precision:** Indicates how many predicted positive cases were actually positive.  
- **Recall:** Indicates how many actual positive cases were correctly identified.  
- **Confusion Matrix:** Displays True Positives, True Negatives, False Positives, and False Negatives.

---

## ğŸ“ˆ Results & Interpretation
The Logistic Regression model performed well on unseen data. Recall was given special importance as it is a critical metric in medical diagnosis problems, where missing positive cases can have serious consequences. The confusion matrix helped analyze classification errors in detail.

---

## ğŸ¯ Key Learnings
- Importance of train-test split to avoid overfitting  
- Understanding and application of evaluation metrics  
- Interpretation of confusion matrix  
- Practical experience in model evaluation  

---

## âœ… Final Outcome
This task helped build a strong foundation in machine learning model evaluation. It provided hands-on experience in training a classification model, testing it on unseen data, and interpreting its performance using standard evaluation techniques.

---
